\documentclass[12pt]{article}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{longtable}
%\usepackage{natbib}
\renewcommand{\thesubsection}{\alph{subsection}.}

\begin{document}
\title{Homework \#5}
\author{Yoav Zimmerman (304125151) \\
	    CS 260: Machine Learning Algorithms \\
	   Collaborators: Hannah Jin, Andrew Wong}
\maketitle

\section{Bias-Variance Tradeoff}
\begin{enumerate}[label=\alph*.]
	\item The closed form of the weights for regularized linear regression takes the following form (derived in previous homeworks and lecture): 
	\begin{gather*}
		\beta_\lambda = (\mathbf{X^T X} + \lambda \mathbf{I})^{-1}  \mathbf{X^T y}
	\end{gather*}
	The training labels $\mathbf{y}$ has a gaussian distribution of $\mathcal{N}( \mathbf{X \beta^*}, \mathbf{\Sigma_\epsilon})$ from the definition of the linear model. We can this distribution and the properites of an affine transformation of a gaussian random variable to find the distribution of $\beta_\lambda$. 
	\begin{gather*}
		\beta_\lambda \sim \mathcal{N}( \mathbf{M_\lambda X \beta^*}, \mathbf{M \Sigma_\epsilon M^T}) \\
		\text{where } \mathbf{M_\lambda} = (\mathbf{X^T X} + \lambda \mathbf{I})^{-1}  \mathbf{X^T}
	\end{gather*}
	
	\item Next, we can calculate the bias term $\mathbb{E}[\mathbf{x^T} \beta_\lambda] - \mathbf{x^T} \beta^*$ as a function of $\lambda$ and a fixed test point \textbf{x}.
	\begin{gather*}
		Bias(\lambda) = \mathbb{E}[\mathbf{x^T} \beta_\lambda] - \mathbf{x^T} \beta^* \\
		Bias(\lambda) = \mathbf{x^T} \mathbb{E}[\beta_\lambda] - \mathbf{x^T} \beta^* \\
		Bias(\lambda) = \mathbf{x^T}  \mathbf{M_\lambda X \beta^*} - \mathbf{x^T} \beta^* \\
		Bias(\lambda) = \mathbf{x^T} ( \mathbf{M_\lambda X} - \mathbf{I} ) \beta^* \\
		\text{where } \mathbf{M_\lambda } = (\mathbf{X^T X} + \lambda \mathbf{I})^{-1}  \mathbf{X^T}
	\end{gather*}
	
	\item TODO:

	\item TODO:
\end{enumerate}

\section{Kernelized Perceptron}
\begin{enumerate}[label=\alph*.]
	\item For this problem, we apply a non-linear feature mapping $\varphi$ to the Perceptron algorithm, resulting in an update rule of $y = sign(\mathbf{w^T} \varphi(\mathbf{x}))$. Therefore, the update rule for a misclassified $y_n$ is the following:
	\begin{gather*}
		\mathbf{w_{i+1}} = \mathbf{w_i} + y_i \varphi(\mathbf{x_i}) \\
	\end{gather*}
	If $\mathbf{w}$ is initialized to zeros, then it immediately follows that it is always a linear combination of the non-linear features vectors:
	\begin{gather*}
		\mathbf{w} = 0 + y_0 \varphi(\mathbf{x_0}) + y_1 \varphi(\mathbf{x_1}) + \dots \\ 
		\mathbf{w} = \sum^N_{i}{\alpha_i \, \varphi(\mathbf{x_i})} \\
	\end{gather*}
	Where $\alpha_i$ is the number of times that the training example $(\mathbf{x_i}, y_i)$ has been misclassified multiplied by the correct class $y_i$.
	
	\item To derive the kernelized form of the prediction rule, we plug the above expression for $\mathbf{w}$ into the prediction function from the regular Perceptron algorithm:
	\begin{gather*}
		y_n = sign(\mathbf{w^T} \varphi(\mathbf{x_n})) \\
		y_n = sign \bigg( \bigg( \sum^N_{i}{\alpha_i \, \varphi(\mathbf{x_i})} \bigg)^T \varphi(\mathbf{x_n}) \bigg) \\ 
		y_n = sign \bigg( \sum^N_{i}{\alpha_i \, \varphi(\mathbf{x_i})^T \, \varphi(\mathbf{x_n})} \bigg) \\
		y_n = sign \bigg( \sum^N_{i}{\alpha_i \, K( \mathbf{x_i}, \mathbf{x_n} )} \bigg)
	\end{gather*}
	
	\item Since $\mathbf{w}$ requires the same number of dimensions as the original features, it does not make sense to store it directly when training with the Perceptron algorithm. Instead we can implicitly use it by maintaining $\alpha_i$ throughout the algorithm. We begin by initializing $\alpha_i$ to zeros, then following the following update rule for every misclassified example:
	\begin{gather*}
		\alpha_{i+1} = \alpha_i + y_i
	\end{gather*}
	The above update rule follows from the following intuition: if $y_n \neq sign \bigg( \sum^N_{i}{\alpha_i \, K( \mathbf{x_i}, \mathbf{x_n} )} \bigg) $,  then 
	\begin{gather*}
		y_n \bigg( \sum^N_{i}{\alpha_i \, K( \mathbf{x_i}, \mathbf{x_n} )} \bigg) < 0
	\end{gather*}
	We want the above value to be positive to correct the misclassification. Now, let us examine the new value after plugging in the new $\alpha_{i+1}$ above
	\begin{gather*}
		y_n \bigg( \sum^N_{i}{(\alpha_i + y_n) K( \mathbf{x_i}, \mathbf{x_n} )} \bigg) \\ 
		y_n \bigg( \sum^N_{i}{\alpha_i \, K( \mathbf{x_i}, \mathbf{x_n} )} \bigg) + y_n \bigg( \sum^N_{i}{y_n \, K( \mathbf{x_i}, \mathbf{x_n} )} \bigg) \\
		y_n \bigg( \sum^N_{i}{\alpha_i \, K( \mathbf{x_i}, \mathbf{x_n} )} \bigg) + y_n^2 \bigg( \sum^N_{i}{ K( \mathbf{x_i}, \mathbf{x_n} )} \bigg)
	\end{gather*}
	Since the kernel function and $y_n^2$ are always greater than zero, we are adding to the original value of the expression and bringing it closer to or above zero.
\end{enumerate}

\section{Kernels}
\begin{enumerate}[label=\alph*.]
	\item Consider a kernel $K_3 = \alpha_1 K_1 + \alpha_2 K_2$ where $K_1$ and $K_2$ are kernel matrices and $\alpha_1 , \alpha_2 \geq 0$. To show that $K_3$ is also a valid kernel, we show that it is positive semidefinite:
	\begin{gather*}
		x^T K_3 x \geq 0 \\
		x^T ( \alpha_1 K_1 + \alpha_2 K_2 ) x \geq 0 \\
		x^T \alpha_1 K_1  x  + x^T \alpha_2 K_2  x \geq 0 \\
	\end{gather*}
	Since  $K_1$ and $K_2$ are positive semidefinite, both terms above are positive, therefore $K_3$ is also positive semidefinite and a kernel matrix.
	
	\item TODO:
	
	\item TODO:
\end{enumerate}

\section{Soft Margin Hyperplanes}
\begin{enumerate}[label=\alph*.]
	\item In the following problem, we consider the following formulation of the SVM optimization problem:
	\begin{equation*}
	\begin{aligned}
	& \underset{w, b, \xi}{\text{min}}
	& & \frac{1}{2} \lVert w \rVert^2_2 + C \sum^N{\xi_n^p} \\
	& \text{subject to}
	& & y_n(w^T \phi(x_n) + b) \geq 1 - \xi_n
	\end{aligned}
	\end{equation*}
	
	To derive the dual formulation, we write out the langragian and take derivatives with respect to the optimization variables $w$, $\beta$, and $\xi$.
	
	\begin{gather*}
		L(w, b, \{ \xi_n \}, \{ \alpha_n \}, \{ \lambda_n \}) = C \sum_n{\xi_n^p} + \frac{1}{2} \lVert w \rVert^2_2 - \sum_n{\lambda_n \, \xi_n} \\
										 + \sum_n{ \bigg( \alpha_n \big( 1 - y_n(w^T \phi(x_n) + b) - \xi_n \big)  \bigg) } 
	\end{gather*}
	
	The partial derivatives with respect to $w$ and $\beta$ are the same as they are with in the standard SVM dual formulation (in the lecture slides), the only partial derivative that is affected by our change is that of $\xi$.
	
	\begin{gather*}
		\frac{ \partial }{ \partial \xi_n } L = 0 \\
		Cp\xi_n^{p-1} - \lambda_n - \alpha_n = 0
	\end{gather*}
	
	When plugging in this system of partial derivatives back into the langragian, we are left with the following dual formulation for an SVM
	
	\begin{equation*}
	\begin{aligned}
	& \underset{\alpha}{\text{max}} 
	& & g ( \{ \alpha_n \}, \{ \lambda_n \} ) = \sum_n{\alpha_n} - \frac{1}{2} \sum_{m, n}{ y_m y_n \alpha_m \alpha_n K(x_m, x_n)} + C \sum_n{ \big( \xi_n - p \xi_n^p \big) } \\
	& \underset{\forall n}{\text{subject to}}
	& & \alpha_n \geq 0 \\
	& & & \sum_n{\alpha_n y_n} = 0 \\
	& & & C_p \xi_n^{p-1} - \lambda_n - \alpha_n = 0 \\
	& & & \lambda_n \geq 0 
	\end{aligned}
	\end{equation*}
	Note that when $p = 1$, the third term in the dual formulation drops out and we're left with the original dual formulation for an SVM.
	
	\item The dual formulation for this more general case is more complex. With $p = 1$, the function to optimize over only has two terms, while in the more general case it has three.
\end{enumerate}

\section{Programming}

\subsection{Data Preprocessing}
The 3rd feature has a mean of 2.4600 and a standard deviation of 1.1223 \\
The 10th feature has a mean of 2.5370 and a standard deviation of 1.1063 \\

\subsection{Cross Validation for Linear SVM}
\begin{enumerate}[label=\alph*.]
	\item The following table shows the average 5-fold cross-validation accuracy and average training time over various values of C. 
	\begin{center}
	\begin{tabular}{| c | c | c |} 
		\hline
	    	C & Average Cross-Validation Accuracy & Average Training Time \\
		\hline \hline
		0.000244 & 0.519000 & 0.862000 \\
		\hline
		0.000977 & 0.784000 & 0.624000 \\
		\hline
		0.003906 & 0.807000 & 0.586000 \\
		\hline
		0.015625 & 0.815000 & 0.672000 \\
		\hline
		0.062500 & 0.807000 & 0.828000 \\
		\hline
		0.250000 & 0.794000 & 0.808000 \\
		\hline
		1.000000 & 0.781000 & 0.876000 \\
		\hline
		4.000000 & 0.784000 & 0.826000 \\
		\hline
		16.000000 & 0.806000 & 0.922000 \\
		\hline
	    \end{tabular}
	\end{center}
	As can be seen, the more extreme values of C result in lower accuracies. If C is too small, then the slack variables aren't allowed to contribute much and the model overfits and becomes too complex. If C is too large, on the other hand, the optimization focuses too much on the slack variables and the model is too simple. A good balance of C is necessary for a successful Soft-Margin SVM. This data does not show a significant relationship between C and training time.
	\item The most successful value for C is \textbf{0.0156}.
	\item Using a value of C = 0.0156 resulted in a test accuracy of \textbf{0.84643678}.
\end{enumerate}

\subsection{Linear SVM in LibSVM}

	The following table shows average 5-fold cross-validation accuracy and average training time over various values of C using LibSVM.
	\begin{center}
	\begin{tabular}{| c | c | c |} 
		\hline
	    	C & Avg Training Time & Avg Accuracy \\
		\hline \hline
		0.000244 & 0.500000 & 51.700000 \\
		\hline
		0.000977 & 0.480000 & 51.700000 \\
		\hline
		0.003906 & 0.520000 & 51.700000 \\
		\hline
		0.015625 & 0.520000 & 51.700000 \\
		\hline
		0.062500 & 0.530000 & 58.400000 \\
		\hline
		0.250000 & 0.480000 & 83.500000 \\
		\hline
		1.000000 & 0.410000 & 87.200000 \\
		\hline
		4.000000 & 0.520000 & 88.500000 \\
		\hline
		16.000000 & 0.500000 & 88.200000 \\
		\hline
	    \end{tabular}
	\end{center}

\begin{enumerate}[label=\alph*.]
	\item As can be seen by the table, the cross-validation accuracy is roughly in the same range as in our Linear SVM implemented from scratch in 5.3. Although LibSVM is solving the dual formulation, the optimization is equivalent.
	\item One difference between our custom implementation and LibSVM is the \textit{training time}. Since LibSVM solves the dual formulation and implements other optimizations, the average training times are roughly half that of our custom implementation solving the primal formulation.
\end{enumerate}

\subsection{Kernel SVM in LibSVM}
\begin{enumerate}[label=\alph*.]
	\item The following table shows the training time and average accuracy over 5-fold cross validation for a Polynomial kernel for various C and degree values.
	\begin{longtable}{| c | c | c | c |} 
		\hline
	    	C & Degree & Avg Training Time & Avg Accuracy \\
		\hline \hline
		0.003906 & 1.000000 & 0.650000 & 51.700000 \\
		\hline
		0.003906 & 2.000000 & 0.480000 & 51.700000 \\
		\hline
		0.003906 & 3.000000 & 0.500000 & 51.700000 \\
		\hline
		0.015625 & 1.000000 & 0.470000 & 51.700000 \\
		\hline
		0.015625 & 2.000000 & 0.480000 & 51.700000 \\
		\hline
		0.015625 & 3.000000 & 0.520000 & 51.700000 \\
		\hline
		0.062500 & 1.000000 & 0.440000 & 78.700000 \\
		\hline
		0.062500 & 2.000000 & 0.510000 & 51.700000 \\
		\hline
		0.062500 & 3.000000 & 0.480000 & 51.700000 \\
		\hline
		0.250000 & 1.000000 & 0.340000 & 80.400000 \\
		\hline
		0.250000 & 2.000000 & 0.490000 & 65.400000 \\
		\hline
		0.250000 & 3.000000 & 0.730000 & 62.100000 \\
		\hline
		1.000000 & 1.000000 & 0.290000 & 79.500000 \\
		\hline
		1.000000 & 2.000000 & 0.600000 & 72.800000 \\
		\hline
		1.000000 & 3.000000 & 0.610000 & 79.200000 \\
		\hline
		4.000000 & 1.000000 & 0.300000 & 79.200000 \\
		\hline
		4.000000 & 2.000000 & 0.570000 & 70.900000 \\
		\hline
		4.000000 & 3.000000 & 0.560000 & 79.000000 \\
		\hline
		16.000000 & 1.000000 & 0.510000 & 79.500000 \\
		\hline
		16.000000 & 2.000000 & 0.600000 & 70.900000 \\
		\hline
		16.000000 & 3.000000 & 0.610000 & 79.200000 \\
		\hline
		64.000000 & 1.000000 & 1.070000 & 79.400000 \\
		\hline
		64.000000 & 2.000000 & 0.630000 & 70.900000 \\
		\hline
		64.000000 & 3.000000 & 0.540000 & 79.200000 \\
		\hline
		256.000000 & 1.000000 & 3.160000 & 79.300000 \\
		\hline
		256.000000 & 2.000000 & 0.540000 & 70.900000 \\
		\hline
		256.000000 & 3.000000 & 0.520000 & 79.200000 \\
		\hline
		1024.000000 & 1.000000 & 11.120000 & 79.200000 \\
		\hline
		1024.000000 & 2.000000 & 0.540000 & 70.900000 \\
		\hline
		1024.000000 & 3.000000 & 0.530000 & 79.200000 \\
		\hline
		4096.000000 & 1.000000 & 44.960000 & 79.300000 \\
		\hline
		4096.000000 & 2.000000 & 0.540000 & 70.900000 \\
		\hline
		4096.000000 & 3.000000 & 0.520000 & 79.200000 \\
		\hline
		16384.000000 & 1.000000 & 85.760000 & 78.800000 \\
		\hline
		16384.000000 & 2.000000 & 0.550000 & 70.900000 \\
		\hline
		16384.000000 & 3.000000 & 0.560000 & 79.200000 \\
		\hline
	\end{longtable}

	\item The following table shows the average training times and accuracies for an RBF kernel trained with various C and gamma values.
	\begin{longtable}{| c | c | c | c |} 
		\hline
	    	C & Gamma & Avg Training Time & Avg Accuracy \\
	    	\hline \hline
		0.003906 & 0.000061 & 0.520000 & 51.700000 \\
		\hline
		0.003906 & 0.000244 & 0.660000 & 51.700000 \\
		\hline
		0.003906 & 0.000977 & 0.610000 & 51.700000 \\
		\hline
		0.003906 & 0.003906 & 0.540000 & 51.700000 \\
		\hline
		0.003906 & 0.015625 & 0.610000 & 51.700000 \\
		\hline
		0.003906 & 0.062500 & 0.710000 & 51.700000 \\
		\hline
		0.003906 & 0.250000 & 0.880000 & 51.700000 \\
		\hline
		0.003906 & 1.000000 & 0.670000 & 51.700000 \\
		\hline
		0.003906 & 4.000000 & 0.700000 & 51.700000 \\
		\hline
		0.003906 & 16.000000 & 0.910000 & 51.700000 \\
		\hline
		0.003906 & 64.000000 & 0.760000 & 51.700000 \\
		\hline
		0.015625 & 0.000061 & 0.880000 & 51.700000 \\
		\hline
		0.015625 & 0.000244 & 0.540000 & 51.700000 \\
		\hline
		0.015625 & 0.000977 & 0.520000 & 51.700000 \\
		\hline
		0.015625 & 0.003906 & 0.510000 & 51.700000 \\
		\hline
		0.015625 & 0.015625 & 0.510000 & 51.700000 \\
		\hline
		0.015625 & 0.062500 & 0.530000 & 51.700000 \\
		\hline
		0.015625 & 0.250000 & 0.540000 & 51.700000 \\
		\hline
		0.015625 & 1.000000 & 0.630000 & 51.700000 \\
		\hline
		0.015625 & 4.000000 & 0.560000 & 51.700000 \\
		\hline
		0.015625 & 16.000000 & 0.540000 & 51.700000 \\
		\hline
		0.015625 & 64.000000 & 0.510000 & 51.700000 \\
		\hline
		0.062500 & 0.000061 & 0.520000 & 51.700000 \\
		\hline
		0.062500 & 0.000244 & 0.550000 & 51.700000 \\
		\hline
		0.062500 & 0.000977 & 0.710000 & 51.700000 \\
		\hline
		0.062500 & 0.003906 & 0.530000 & 54.500000 \\
		\hline
		0.062500 & 0.015625 & 0.580000 & 61.200000 \\
		\hline
		0.062500 & 0.062500 & 0.590000 & 51.700000 \\
		\hline
		0.062500 & 0.250000 & 0.570000 & 51.700000 \\
		\hline
		0.062500 & 1.000000 & 0.760000 & 51.700000 \\
		\hline
		0.062500 & 4.000000 & 0.720000 & 51.700000 \\
		\hline
		0.062500 & 16.000000 & 0.540000 & 51.700000 \\
		\hline
		0.062500 & 64.000000 & 0.510000 & 51.700000 \\
		\hline
		0.250000 & 0.000061 & 0.630000 & 51.700000 \\
		\hline
		0.250000 & 0.000244 & 0.620000 & 51.700000 \\
		\hline
		0.250000 & 0.000977 & 0.560000 & 73.200000 \\
		\hline
		0.250000 & 0.003906 & 0.550000 & 80.300000 \\
		\hline
		0.250000 & 0.015625 & 0.620000 & 83.500000 \\
		\hline
		0.250000 & 0.062500 & 0.580000 & 51.700000 \\
		\hline
		0.250000 & 0.250000 & 0.680000 & 51.700000 \\
		\hline
		0.250000 & 1.000000 & 0.720000 & 51.700000 \\
		\hline
		0.250000 & 4.000000 & 0.590000 & 51.700000 \\
		\hline
		0.250000 & 16.000000 & 0.730000 & 51.700000 \\
		\hline
		0.250000 & 64.000000 & 0.560000 & 51.700000 \\
		\hline
		1.000000 & 0.000061 & 0.500000 & 51.700000 \\
		\hline
		1.000000 & 0.000244 & 0.580000 & 75.900000 \\
		\hline
		1.000000 & 0.000977 & 0.520000 & 79.900000 \\
		\hline
		1.000000 & 0.003906 & 0.440000 & 81.700000 \\
		\hline
		1.000000 & 0.015625 & 0.440000 & 87.200000 \\
		\hline
		1.000000 & 0.062500 & 0.590000 & 76.600000 \\
		\hline
		1.000000 & 0.250000 & 0.600000 & 57.300000 \\
		\hline
		1.000000 & 1.000000 & 0.730000 & 55.800000 \\
		\hline
		1.000000 & 4.000000 & 0.640000 & 55.200000 \\
		\hline
		1.000000 & 16.000000 & 0.590000 & 55.200000 \\
		\hline
		1.000000 & 64.000000 & 0.580000 & 55.200000 \\
		\hline
		4.000000 & 0.000061 & 0.520000 & 77.000000 \\
		\hline
		4.000000 & 0.000244 & 0.470000 & 79.700000 \\
		\hline
		4.000000 & 0.000977 & 0.390000 & 80.000000 \\
		\hline
		4.000000 & 0.003906 & 0.330000 & 84.700000 \\
		\hline
		4.000000 & 0.015625 & 0.530000 & 88.600000 \\
		\hline
		4.000000 & 0.062500 & 0.620000 & 77.900000 \\
		\hline
		4.000000 & 0.250000 & 0.600000 & 57.900000 \\
		\hline
		4.000000 & 1.000000 & 0.860000 & 56.200000 \\
		\hline
		4.000000 & 4.000000 & 0.690000 & 55.200000 \\
		\hline
		4.000000 & 16.000000 & 0.580000 & 55.200000 \\
		\hline
		4.000000 & 64.000000 & 0.580000 & 55.200000 \\
		\hline
		16.000000 & 0.000061 & 0.430000 & 79.500000 \\
		\hline
		16.000000 & 0.000244 & 0.350000 & 79.700000 \\
		\hline
		16.000000 & 0.000977 & 0.390000 & 81.100000 \\
		\hline
		16.000000 & 0.003906 & 0.440000 & 88.000000 \\
		\hline
		16.000000 & 0.015625 & 0.500000 & 88.200000 \\
		\hline
		16.000000 & 0.062500 & 0.650000 & 77.900000 \\
		\hline
		16.000000 & 0.250000 & 0.590000 & 57.900000 \\
		\hline
		16.000000 & 1.000000 & 0.840000 & 56.200000 \\
		\hline
		16.000000 & 4.000000 & 0.600000 & 55.200000 \\
		\hline
		16.000000 & 16.000000 & 0.550000 & 55.200000 \\
		\hline
		16.000000 & 64.000000 & 0.570000 & 55.200000 \\
		\hline
		64.000000 & 0.000061 & 0.350000 & 79.800000 \\
		\hline
		64.000000 & 0.000244 & 0.340000 & 79.600000 \\
		\hline
		64.000000 & 0.000977 & 0.360000 & 85.700000 \\
		\hline
		64.000000 & 0.003906 & 0.520000 & 87.200000 \\
		\hline
		64.000000 & 0.015625 & 0.500000 & 88.200000 \\
		\hline
		64.000000 & 0.062500 & 0.570000 & 77.900000 \\
		\hline
		64.000000 & 0.250000 & 0.570000 & 57.900000 \\
		\hline
		64.000000 & 1.000000 & 0.770000 & 56.200000 \\
		\hline
		64.000000 & 4.000000 & 0.630000 & 55.200000 \\
		\hline
		64.000000 & 16.000000 & 0.590000 & 55.200000 \\
		\hline
		64.000000 & 64.000000 & 0.550000 & 55.200000 \\
		\hline
		256.000000 & 0.000061 & 0.320000 & 79.000000 \\
		\hline
		256.000000 & 0.000244 & 0.380000 & 81.700000 \\
		\hline
		256.000000 & 0.000977 & 0.610000 & 88.100000 \\
		\hline
		256.000000 & 0.003906 & 0.520000 & 87.200000 \\
		\hline
		256.000000 & 0.015625 & 0.520000 & 88.200000 \\
		\hline
		256.000000 & 0.062500 & 0.610000 & 77.900000 \\
		\hline
		256.000000 & 0.250000 & 0.580000 & 57.900000 \\
		\hline
		256.000000 & 1.000000 & 0.820000 & 56.200000 \\
		\hline
		256.000000 & 4.000000 & 0.590000 & 55.200000 \\
		\hline
		256.000000 & 16.000000 & 0.580000 & 55.200000 \\
		\hline
		256.000000 & 64.000000 & 0.560000 & 55.200000 \\
		\hline
		1024.000000 & 0.000061 & 0.370000 & 79.400000 \\
		\hline
		1024.000000 & 0.000244 & 0.640000 & 85.400000 \\
		\hline
		1024.000000 & 0.000977 & 0.820000 & 87.400000 \\
		\hline
		1024.000000 & 0.003906 & 0.520000 & 87.200000 \\
		\hline
		1024.000000 & 0.015625 & 0.520000 & 88.200000 \\
		\hline
		1024.000000 & 0.062500 & 0.570000 & 77.900000 \\
		\hline
		1024.000000 & 0.250000 & 0.560000 & 57.900000 \\
		\hline
		1024.000000 & 1.000000 & 0.800000 & 56.200000 \\
		\hline
		1024.000000 & 4.000000 & 0.600000 & 55.200000 \\
		\hline
		1024.000000 & 16.000000 & 0.580000 & 55.200000 \\
		\hline
		1024.000000 & 64.000000 & 0.560000 & 55.200000 \\
		\hline
		4096.000000 & 0.000061 & 0.570000 & 81.800000 \\
		\hline
		4096.000000 & 0.000244 & 1.530000 & 87.800000 \\
		\hline
		4096.000000 & 0.000977 & 0.770000 & 87.400000 \\
		\hline
		4096.000000 & 0.003906 & 0.540000 & 87.200000 \\
		\hline
		4096.000000 & 0.015625 & 0.580000 & 88.200000 \\
		\hline
		4096.000000 & 0.062500 & 0.650000 & 77.900000 \\
		\hline
		4096.000000 & 0.250000 & 0.640000 & 57.900000 \\
		\hline
		4096.000000 & 1.000000 & 0.800000 & 56.200000 \\
		\hline
		4096.000000 & 4.000000 & 0.590000 & 55.200000 \\
		\hline
		4096.000000 & 16.000000 & 0.620000 & 55.200000 \\
		\hline
		4096.000000 & 64.000000 & 0.550000 & 55.200000 \\
		\hline
		16384.000000 & 0.000061 & 1.480000 & 85.400000 \\
		\hline
		16384.000000 & 0.000244 & 2.120000 & 87.400000 \\
		\hline
		16384.000000 & 0.000977 & 0.740000 & 87.400000 \\
		\hline
		16384.000000 & 0.003906 & 0.550000 & 87.200000 \\
		\hline
		16384.000000 & 0.015625 & 0.520000 & 88.200000 \\
		\hline
		16384.000000 & 0.062500 & 0.570000 & 77.900000 \\
		\hline
		16384.000000 & 0.250000 & 0.550000 & 57.900000 \\
		\hline
		16384.000000 & 1.000000 & 0.770000 & 56.200000 \\
		\hline
		16384.000000 & 4.000000 & 0.580000 & 55.200000 \\
		\hline
		16384.000000 & 16.000000 & 0.540000 & 55.200000 \\
		\hline
		16384.000000 & 64.000000 & 0.560000 & 55.200000 \\
		\hline
	    \end{longtable}


\item The best accuracy achieved over all of these different possible kernels is an RBF kernel with a C value of \textbf{4} and a gamma value of \textbf{0.0156}. Training a model with these kernel and parameter settings, we are able to reach a Test Accuracy of \textbf{90.43678161} on the dataset! Not bad.

\end{enumerate}

%\bibliographystyle{plain}
%\bibliography{refs}

\end{document}